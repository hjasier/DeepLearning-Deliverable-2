{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the movieLens dataset\n",
    "\n",
    "u.data consists on 4 columns : user_id , movie_id , rating and timestamp\n",
    "\n",
    "u.genre    -- A list of the genres.\n",
    "\n",
    "u.user     -- Demographic information about the users; this is a tab\n",
    "              separated list of\n",
    "              user id | age | gender | occupation | zip code\n",
    "              The user ids are the ones used in the u.data data set.\n",
    "\n",
    "u.occupation -- A list of the occupations.\n",
    "\n",
    "\n",
    "u.info     -- The number of users, items, and ratings in the u data set.\n",
    "\n",
    "u.item     -- Information about the items (movies); this is a tab separated\n",
    "              list of\n",
    "              movie id | movie title | release date | video release date |\n",
    "              IMDb URL | unknown | Action | Adventure | Animation |\n",
    "              Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "              Thriller | War | Western |\n",
    "              The last 19 fields are the genres, a 1 indicates the movie\n",
    "              is of that genre, a 0 indicates it is not; movies can be in\n",
    "              several genres at once.\n",
    "              The movie ids are the ones used in the u.data data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:\n",
      "   user_id  movie_id  rating  timestamp\n",
      "0      196       242       3  881250949\n",
      "1      186       302       3  891717742\n",
      "2       22       377       1  878887116\n",
      "3      244        51       2  880606923\n",
      "4      166       346       1  886397596\n",
      "\n",
      "Movies:\n",
      "   movie_id              title release_date  video_release_date  \\\n",
      "0         1   Toy Story (1995)  01-Jan-1995                 NaN   \n",
      "1         2   GoldenEye (1995)  01-Jan-1995                 NaN   \n",
      "2         3  Four Rooms (1995)  01-Jan-1995                 NaN   \n",
      "3         4  Get Shorty (1995)  01-Jan-1995                 NaN   \n",
      "4         5     Copycat (1995)  01-Jan-1995                 NaN   \n",
      "\n",
      "                                            IMDb_URL  genre_0  genre_1  \\\n",
      "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...        0        0   \n",
      "1  http://us.imdb.com/M/title-exact?GoldenEye%20(...        0        1   \n",
      "2  http://us.imdb.com/M/title-exact?Four%20Rooms%...        0        0   \n",
      "3  http://us.imdb.com/M/title-exact?Get%20Shorty%...        0        1   \n",
      "4  http://us.imdb.com/M/title-exact?Copycat%20(1995)        0        0   \n",
      "\n",
      "   genre_2  genre_3  genre_4  ...  genre_9  genre_10  genre_11  genre_12  \\\n",
      "0        0        1        1  ...        0         0         0         0   \n",
      "1        1        0        0  ...        0         0         0         0   \n",
      "2        0        0        0  ...        0         0         0         0   \n",
      "3        0        0        0  ...        0         0         0         0   \n",
      "4        0        0        0  ...        0         0         0         0   \n",
      "\n",
      "   genre_13  genre_14  genre_15  genre_16  genre_17  genre_18  \n",
      "0         0         0         0         0         0         0  \n",
      "1         0         0         0         1         0         0  \n",
      "2         0         0         0         1         0         0  \n",
      "3         0         0         0         0         0         0  \n",
      "4         0         0         0         1         0         0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Users:\n",
      "   user_id  age gender  occupation zip_code\n",
      "0        1   24      M  technician    85711\n",
      "1        2   53      F       other    94043\n",
      "2        3   23      M      writer    32067\n",
      "3        4   24      M  technician    43537\n",
      "4        5   33      F       other    15213\n"
     ]
    }
   ],
   "source": [
    "data_path = \"ml-100k/u.data\"\n",
    "item_path = \"ml-100k/u.item\"\n",
    "user_path = \"ml-100k/u.user\"\n",
    "\n",
    "# Load u.data\n",
    "columns = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
    "ratings = pd.read_csv(data_path, sep=\"\\t\", names=columns, encoding=\"latin-1\")\n",
    "\n",
    "# Load u.item\n",
    "item_columns = [\"movie_id\", \"title\", \"release_date\", \"video_release_date\", \"IMDb_URL\"] + [f\"genre_{i}\" for i in range(19)]\n",
    "movies = pd.read_csv(item_path, sep=\"|\", names=item_columns, encoding=\"latin-1\", usecols=range(24))\n",
    "\n",
    "# Load u.user\n",
    "user_columns = [\"user_id\", \"age\", \"gender\", \"occupation\", \"zip_code\"]\n",
    "users = pd.read_csv(user_path, sep=\"|\", names=user_columns, encoding=\"latin-1\")\n",
    "\n",
    "\n",
    "print(\"Ratings:\")\n",
    "print(ratings.head())\n",
    "print(\"\\nMovies:\")\n",
    "print(movies.head())\n",
    "print(\"\\nUsers:\")\n",
    "print(users.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de usuarios: 943\n",
      "N√∫mero de pel√≠culas: 1682\n",
      "N√∫mero de ratings: 100000\n",
      "\n",
      "Distribuci√≥n de ratings:\n",
      "rating\n",
      "4    34174\n",
      "3    27145\n",
      "5    21201\n",
      "2    11370\n",
      "1     6110\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuci√≥n de edades de usuarios:\n",
      "count    943.000000\n",
      "mean      34.051962\n",
      "std       12.192740\n",
      "min        7.000000\n",
      "25%       25.000000\n",
      "50%       31.000000\n",
      "75%       43.000000\n",
      "max       73.000000\n",
      "Name: age, dtype: float64\n",
      "\n",
      "Pel√≠culas con m√°s ratings:\n",
      "movie_id\n",
      "50     583\n",
      "258    509\n",
      "100    508\n",
      "181    507\n",
      "294    485\n",
      "286    481\n",
      "288    478\n",
      "1      452\n",
      "300    431\n",
      "121    429\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"N√∫mero de usuarios: {ratings['user_id'].nunique()}\")\n",
    "print(f\"N√∫mero de pel√≠culas: {ratings['movie_id'].nunique()}\")\n",
    "print(f\"N√∫mero de ratings: {ratings.shape[0]}\")\n",
    "\n",
    "print(\"\\nDistribuci√≥n de ratings:\")\n",
    "\n",
    "print(ratings['rating'].value_counts())\n",
    "\n",
    "print(\"\\nDistribuci√≥n de edades de usuarios:\")\n",
    "print(users['age'].describe())\n",
    "\n",
    "print(\"\\nPel√≠culas con m√°s ratings:\")\n",
    "print(ratings['movie_id'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "### Ratings (u.data)\n",
    "\n",
    "- There are 943 users and 1,682 movies with 100,000 ratings.\n",
    "- Most ratings are 4 and 3, indicating a tendency to rate higher rather than lower.\n",
    "- There are fewer ratings of 1 and 2, which could mean that users prefer not to rate bad movies rather than giving low scores.\n",
    "\n",
    "### Users (u.user)\n",
    "\n",
    "- Average age: 34 years.\n",
    "- Minimum age: 7 years, maximum age: 73 years.\n",
    "- Most users are between 25 and 43 years old (25%-75% percentiles).\n",
    "\n",
    "### Movies (u.item)\n",
    "\n",
    "- The most-rated movie has 583 ratings, while many others have very few ratings.\n",
    "- This indicates an issue with imbalanced data: some movies are very popular, while others receive almost no ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load genre Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          genre  genre_id\n",
      "0       unknown         0\n",
      "1        Action         1\n",
      "2     Adventure         2\n",
      "3     Animation         3\n",
      "4    Children's         4\n",
      "5        Comedy         5\n",
      "6         Crime         6\n",
      "7   Documentary         7\n",
      "8         Drama         8\n",
      "9       Fantasy         9\n",
      "10    Film-Noir        10\n",
      "11       Horror        11\n",
      "12      Musical        12\n",
      "13      Mystery        13\n",
      "14      Romance        14\n",
      "15       Sci-Fi        15\n",
      "16     Thriller        16\n",
      "17          War        17\n",
      "18      Western        18\n"
     ]
    }
   ],
   "source": [
    "genre_labels =  pd.read_csv(\"ml-100k/u.genre\", sep=\"|\", names=[\"genre\",\"genre_id\"], encoding=\"latin-1\")\n",
    "print(genre_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratings:\n",
      "   user_id  movie_id  rating\n",
      "0      196       242       3\n",
      "1      186       302       3\n",
      "2       22       377       1\n",
      "3      244        51       2\n",
      "4      166       346       1\n",
      "\n",
      "Users:\n",
      "   user_id  age  gender  occupation zip_code\n",
      "0        1   24       1  technician    85711\n",
      "1        2   53       0       other    94043\n",
      "2        3   23       1      writer    32067\n",
      "3        4   24       1  technician    43537\n",
      "4        5   33       0       other    15213\n",
      "\n",
      "Movies:\n",
      "   movie_id              title release_date  video_release_date  \\\n",
      "0         1   Toy Story (1995)  01-Jan-1995                 NaN   \n",
      "1         2   GoldenEye (1995)  01-Jan-1995                 NaN   \n",
      "2         3  Four Rooms (1995)  01-Jan-1995                 NaN   \n",
      "3         4  Get Shorty (1995)  01-Jan-1995                 NaN   \n",
      "4         5     Copycat (1995)  01-Jan-1995                 NaN   \n",
      "\n",
      "                                            IMDb_URL  \\\n",
      "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...   \n",
      "1  http://us.imdb.com/M/title-exact?GoldenEye%20(...   \n",
      "2  http://us.imdb.com/M/title-exact?Four%20Rooms%...   \n",
      "3  http://us.imdb.com/M/title-exact?Get%20Shorty%...   \n",
      "4  http://us.imdb.com/M/title-exact?Copycat%20(1995)   \n",
      "\n",
      "                            genres  \n",
      "0  [Animation, Children's, Comedy]  \n",
      "1    [Action, Adventure, Thriller]  \n",
      "2                       [Thriller]  \n",
      "3          [Action, Comedy, Drama]  \n",
      "4         [Crime, Drama, Thriller]  \n"
     ]
    }
   ],
   "source": [
    "# Timestamp column is not useful\n",
    "ratings = ratings.drop(columns=[\"timestamp\"])\n",
    "\n",
    "# Convert gender to binary\n",
    "users[\"gender\"] = users[\"gender\"].map({\"M\": 1, \"F\": 0})\n",
    "\n",
    "\n",
    "genre_labels = genre_labels[\"genre\"].tolist()\n",
    "\n",
    "# Convertir g√©neros de 0s y 1s a listas de nombres\n",
    "movies[\"genres\"] = movies.apply(lambda row: [genre_labels[i] for i in range(19) if row[f\"genre_{i}\"] == 1], axis=1)\n",
    "\n",
    "# Eliminar columnas de g√©nero codificado\n",
    "movies = movies.drop(columns=[f\"genre_{i}\" for i in range(19)])\n",
    "\n",
    "# Mostrar cambios\n",
    "print(\"\\nRatings:\")\n",
    "print(ratings.head())\n",
    "print(\"\\nUsers:\")\n",
    "print(users.head())\n",
    "print(\"\\nMovies:\")\n",
    "print(movies.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificar los IDs (user_id, movie_id)\n",
    "\n",
    "PyTorch trabaja mejor con √≠ndices consecutivos en vez de n√∫meros aleatorios. Vamos a:\n",
    "\n",
    "    Mapear user_id y movie_id a √≠ndices consecutivos.\n",
    "\n",
    "    Guardar el n√∫mero total de usuarios y pel√≠culas para la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usuarios: 943, Total pel√≠culas: 1682\n",
      "   user_id  movie_id  rating\n",
      "0      195       241       3\n",
      "1      185       301       3\n",
      "2       21       376       1\n",
      "3      243        50       2\n",
      "4      165       345       1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Crear mappings de ID a √≠ndice\n",
    "user2idx = {user_id: idx for idx, user_id in enumerate(users[\"user_id\"].unique())}\n",
    "movie2idx = {movie_id: idx for idx, movie_id in enumerate(movies[\"movie_id\"].unique())}\n",
    "\n",
    "# Aplicar mapeo\n",
    "ratings[\"user_id\"] = ratings[\"user_id\"].map(user2idx)\n",
    "ratings[\"movie_id\"] = ratings[\"movie_id\"].map(movie2idx)\n",
    "\n",
    "# Guardar n√∫mero total de usuarios y pel√≠culas\n",
    "num_users = len(user2idx)\n",
    "num_movies = len(movie2idx)\n",
    "\n",
    "print(f\"Total usuarios: {num_users}, Total pel√≠culas: {num_movies}\")\n",
    "print(ratings.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir en Train / Validation / Test\n",
    "\n",
    "    Train (70%) ‚Üí Para entrenar el modelo.\n",
    "\n",
    "    Validation (15%) ‚Üí Para ajustar hiperpar√°metros.\n",
    "\n",
    "    Test (15%) ‚Üí Para evaluar el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o Train: 70000, Validaci√≥n: 15000, Test: 15000\n"
     ]
    }
   ],
   "source": [
    "# Dividir en train (70%), test (15%), validation (15%)\n",
    "train_data, temp_data = train_test_split(ratings, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Tama√±o Train: {len(train_data)}, Validaci√≥n: {len(val_data)}, Test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear PyTorch Dataset y DataLoader\n",
    "\n",
    "Para entrenar una red neuronal en PyTorch, necesitamos:\n",
    "\n",
    "‚úÖ Crear un Dataset que convierta nuestros datos en tensores.\n",
    "\n",
    "‚úÖ Usar DataLoader para cargar los datos en batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch de usuarios: tensor([ 23, 847, 601, 311, 810])\n",
      "Batch de pel√≠culas: tensor([356, 133, 180,   0, 891])\n",
      "Batch de ratings: tensor([5., 5., 5., 5., 4.])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define our dataset class \n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df[\"user_id\"].values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(df[\"movie_id\"].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df[\"rating\"].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = MovieLensDataset(train_data)\n",
    "val_dataset = MovieLensDataset(val_data)\n",
    "test_dataset = MovieLensDataset(test_data)\n",
    "\n",
    "# Crear DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verificar que funciona\n",
    "for users, movies, ratings in train_loader:\n",
    "    print(\"Batch de usuarios:\", users[:5])\n",
    "    print(\"Batch de pel√≠culas:\", movies[:5])\n",
    "    print(\"Batch de ratings:\", ratings[:5])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèó Paso 4: Dise√±ar la Red Neuronal\n",
    "\n",
    "Vamos a usar una Embedding Neural Network, que es com√∫n en sistemas de recomendaci√≥n. La estructura ser√°:\n",
    "\n",
    "1Ô∏è‚É£ Capa de embeddings para representar usuarios y pel√≠culas en un espacio de caracter√≠sticas.\n",
    "\n",
    "2Ô∏è‚É£ Capas completamente conectadas (Fully Connected, FC) para capturar interacciones no lineales.\n",
    "\n",
    "3Ô∏è‚É£ Salida con una √∫nica neurona que predice la calificaci√≥n del usuario para la pel√≠cula.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "üìö Explicaci√≥n de la arquitectura de RecommenderNet\n",
    "\n",
    "Este modelo est√° basado en una red neuronal de recomendaciones utilizando embeddings. Los embeddings son una forma eficiente de representar elementos (en este caso, usuarios y pel√≠culas) en un espacio vectorial denso de menor dimensi√≥n, donde las relaciones sem√°nticas entre estos elementos se pueden aprender.\n",
    "1Ô∏è‚É£ Embeddings\n",
    "\n",
    "    self.user_embedding y self.movie_embedding: Son capas de embeddings que transforman los identificadores de usuario y pel√≠cula en vectores de caracter√≠sticas de tama√±o embedding_dim. Cada usuario y cada pel√≠cula se mapea a un vector de caracter√≠sticas en un espacio de dimensi√≥n embedding_dim (en este caso, 50).\n",
    "\n",
    "        El objetivo de los embeddings es que usuarios similares y pel√≠culas similares tengan representaciones vectoriales cercanas en ese espacio.\n",
    "\n",
    "        La dimensi√≥n de los embeddings es un hiperpar√°metro importante: si es demasiado peque√±o, el modelo puede no capturar toda la informaci√≥n; si es demasiado grande, el modelo puede volverse m√°s complejo y propenso a sobreajustarse.\n",
    "\n",
    "2Ô∏è‚É£ Capas totalmente conectadas (Fully Connected Layers)\n",
    "\n",
    "    Despu√©s de obtener las representaciones de los usuarios y las pel√≠culas a trav√©s de los embeddings, los concatenamos en un solo vector de tama√±o 2 * embedding_dim (en este caso, 2 * 50 = 100).\n",
    "\n",
    "        fc1 toma esta concatenaci√≥n y la mapea a un espacio de tama√±o 128. Este es el primer capa densa. Eleg√≠ 128 porque es un buen valor intermedio que permite suficiente capacidad para aprender relaciones no lineales sin ser demasiado grande, lo que podr√≠a causar sobreajuste.\n",
    "\n",
    "        fc2 reduce la dimensi√≥n a 64. Esto hace que el modelo se haga m√°s compacto y pueda aprender representaciones m√°s generalizadas de las interacciones entre usuarios y pel√≠culas. 64 es otra elecci√≥n intermedia para evitar que el modelo sea demasiado grande, pero a√∫n as√≠ tenga suficiente capacidad para aprender.\n",
    "\n",
    "        fc3 produce la salida final. Dado que estamos prediciendo una calificaci√≥n, solo necesitamos una neurona en esta capa, que es la que nos da el rating estimado entre 1 y 5 (en este caso, valores continuos).\n",
    "\n",
    "3Ô∏è‚É£ Funciones de activaci√≥n\n",
    "\n",
    "    ReLU: Se usa para introducir no linealidades entre las capas. Esta funci√≥n activa los valores positivos y \"apaga\" los negativos (los pone en cero). De esta manera, el modelo puede aprender patrones complejos.\n",
    "\n",
    "4Ô∏è‚É£ Inicializaci√≥n de pesos\n",
    "\n",
    "    _init_weights: Aqu√≠ inicializamos los pesos de las redes neuronales de manera que favorezcan el aprendizaje, usando una combinaci√≥n de inicializaci√≥n normal y uniforme de He para las capas totalmente conectadas. Esto ayuda a prevenir problemas de saturaci√≥n en las activaciones.\n",
    "\n",
    "üîß Elecci√≥n de los hiperpar√°metros\n",
    "1Ô∏è‚É£ embedding_dim = 50\n",
    "\n",
    "    ¬øPor qu√© 50?\n",
    "\n",
    "        Eleg√≠ 50 como valor inicial para el tama√±o de los embeddings, ya que es un tama√±o razonable para representar informaci√≥n de usuarios y pel√≠culas en un espacio compacto sin perder mucha capacidad de aprendizaje.\n",
    "\n",
    "        Si fuera mucho mayor, los embeddings tendr√≠an demasiados par√°metros, lo que podr√≠a hacer que el modelo sea m√°s complejo y propenso a sobreajustarse.\n",
    "\n",
    "        Si fuera mucho menor, podr√≠a no ser capaz de capturar adecuadamente la complejidad de las relaciones entre los usuarios y las pel√≠culas.\n",
    "\n",
    "        50 es un valor equilibrado que suele ser suficiente para estos tipos de tareas de recomendaci√≥n.\n",
    "\n",
    "2Ô∏è‚É£ Capas completamente conectadas:\n",
    "\n",
    "    fc1 = 128, fc2 = 64:\n",
    "\n",
    "        Estas capas permiten que el modelo capture interacciones m√°s complejas entre los usuarios y las pel√≠culas.\n",
    "\n",
    "        Eleg√≠ 128 y 64 porque son valores suficientemente grandes como para aprender relaciones no triviales entre los datos. Sin embargo, tambi√©n son valores moderados, lo que ayuda a evitar que el modelo se haga demasiado complejo (lo cual podr√≠a generar sobreajuste) mientras sigue siendo lo suficientemente potente para capturar patrones.\n",
    "\n",
    "3Ô∏è‚É£ Capa de salida (fc3):\n",
    "\n",
    "    La capa de salida tiene un solo nodo porque estamos prediciendo un valor continuo (la calificaci√≥n que un usuario le da a una pel√≠cula). El valor predicho estar√° entre 1 y 5, lo que corresponde a la calificaci√≥n de la pel√≠cula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RecommenderNet(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=100):\n",
    "        super(RecommenderNet, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)  # Salida: un n√∫mero (rating)\n",
    "        \n",
    "        # Inicializaci√≥n de pesos\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.movie_embedding.weight, std=0.01)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight)\n",
    "    \n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        # Obtener embeddings\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        movie_vec = self.movie_embedding(movie_ids)\n",
    "        \n",
    "        # Concatenar embeddings\n",
    "        x = torch.cat([user_vec, movie_vec], dim=1)\n",
    "        \n",
    "        # Pasar por capas densas\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x.squeeze()  # Salida final: un valor escalar (rating)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèãÔ∏è‚Äç‚ôÇÔ∏è Paso 5: Entrenar el Modelo\n",
    "\n",
    "Ahora que hemos definido la arquitectura, necesitamos:\n",
    "\n",
    "    Definir la funci√≥n de p√©rdida (Loss Function): Ya que estamos prediciendo calificaciones, la funci√≥n de p√©rdida que vamos a usar es el Error Cuadr√°tico Medio (MSE, por sus siglas en ingl√©s).\n",
    "\n",
    "    Definir el optimizador: Vamos a usar el optimizador Adam, que es uno de los m√°s populares para este tipo de tareas y funciona bien para redes neuronales de recomendaci√≥n.\n",
    "\n",
    "    Entrenar el modelo: Ajustar los pesos del modelo usando el conjunto de entrenamiento, validaci√≥n y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6 - Train Loss: 1.4613 - Validation Loss: 0.9103\n",
      "Mejor modelo guardado.\n",
      "Epoch 2/6 - Train Loss: 0.8791 - Validation Loss: 0.8959\n",
      "Mejor modelo guardado.\n",
      "Epoch 3/6 - Train Loss: 0.8415 - Validation Loss: 0.8815\n",
      "Mejor modelo guardado.\n",
      "Epoch 4/6 - Train Loss: 0.8117 - Validation Loss: 0.8760\n",
      "Mejor modelo guardado.\n",
      "Epoch 5/6 - Train Loss: 0.7733 - Validation Loss: 0.8710\n",
      "Mejor modelo guardado.\n",
      "Epoch 6/6 - Train Loss: 0.7301 - Validation Loss: 0.8677\n",
      "Mejor modelo guardado.\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Inicializamos el modelo\n",
    "model = RecommenderNet(num_users, num_movies, embedding_dim=100)\n",
    "\n",
    "# Definir la funci√≥n de p√©rdida y el optimizador\n",
    "criterion = nn.MSELoss()  # Error cuadr√°tico medio\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Funci√≥n de entrenamiento con Early Stopping\n",
    "def train_model_early_stopping(model, train_loader, val_loader, criterion, optimizer, epochs=30, patience=5):\n",
    "    best_val_loss = float('inf')  # Mantener la mejor p√©rdida de validaci√≥n\n",
    "    epochs_without_improvement = 0  # Contador de √©pocas sin mejora\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Poner el modelo en modo de entrenamiento\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for users, movies, ratings in train_loader:\n",
    "            optimizer.zero_grad()  # Limpiar los gradientes\n",
    "            \n",
    "            # Hacer las predicciones\n",
    "            predictions = model(users, movies)\n",
    "            \n",
    "            # Calcular la p√©rdida\n",
    "            loss = criterion(predictions, ratings)\n",
    "            \n",
    "            # Hacer backpropagation y actualizar los pesos\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Promediar la p√©rdida del entrenamiento\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validaci√≥n\n",
    "        model.eval()  # Modo evaluaci√≥n\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for users, movies, ratings in val_loader:\n",
    "                predictions = model(users, movies)\n",
    "                loss = criterion(predictions, ratings)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Imprimir resultados\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Guardar el modelo si la p√©rdida de validaci√≥n mejora\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Mejor modelo guardado.\")\n",
    "            epochs_without_improvement = 0  # Resetear el contador\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"No hubo mejora en la p√©rdida de validaci√≥n, deteniendo el entrenamiento.\")\n",
    "                break\n",
    "    \n",
    "    print(\"Entrenamiento completado.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Entrenar el modelo\n",
    "epochs = 6  \n",
    "trained_model = train_model_early_stopping(model, train_loader, val_loader, criterion, optimizer, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8745 - MSE: 0.8742\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # Establecer el modelo en modo evaluaci√≥n\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings in test_loader:\n",
    "            # Hacer las predicciones\n",
    "            predictions = model(users, movies)\n",
    "            \n",
    "            # Calcular la p√©rdida\n",
    "            loss = criterion(predictions, ratings)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Almacenar las predicciones y las etiquetas reales para calcular m√©tricas\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(ratings.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f} - MSE: {mse:.4f}\")\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_mse = evaluate_model(trained_model, test_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
