{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#importar F.leaky_relu\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the movieLens dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:\n",
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "\n",
      "Movies:\n",
      "   MovieID                               Title                        Genres\n",
      "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4        5  Father of the Bride Part II (1995)                        Comedy\n",
      "\n",
      "Users:\n",
      "   UserID Gender  Age  Occupation Zip-code\n",
      "0       1      F    1          10    48067\n",
      "1       2      M   56          16    70072\n",
      "2       3      M   25          15    55117\n",
      "3       4      M   45           7    02460\n",
      "4       5      M   25          20    55455\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rutas de los archivos\n",
    "movies_path = \"ml-1m/movies.dat\"\n",
    "ratings_path = \"ml-1m/ratings.dat\"\n",
    "users_path = \"ml-1m/users.dat\"\n",
    "\n",
    "# Carga de los datos\n",
    "users = pd.read_csv(users_path, sep=\"::\", engine=\"python\", names=[\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"], encoding=\"latin-1\")\n",
    "movies = pd.read_csv(movies_path, sep=\"::\", engine=\"python\", names=[\"MovieID\", \"Title\", \"Genres\"], encoding=\"latin-1\")\n",
    "ratings = pd.read_csv(ratings_path, sep=\"::\", engine=\"python\", names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], encoding=\"latin-1\")\n",
    "\n",
    "# Mostrar primeras filas para verificar\n",
    "print(\"Ratings:\")\n",
    "print(ratings.head())\n",
    "print(\"\\nMovies:\")\n",
    "print(movies.head())\n",
    "print(\"\\nUsers:\")\n",
    "print(users.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MovieID                               Title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                             Genres  \n",
      "0   [Animation, Children's, Comedy]  \n",
      "1  [Adventure, Children's, Fantasy]  \n",
      "2                 [Comedy, Romance]  \n",
      "3                   [Comedy, Drama]  \n",
      "4                          [Comedy]  \n"
     ]
    }
   ],
   "source": [
    "# Convertir géneros a lista de géneros\n",
    "movies['Genres'] = movies['Genres'].apply(lambda x: x.split('|'))\n",
    "\n",
    "# Verificar cambios\n",
    "print(movies.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usuarios: 6040, Total películas: 3883\n",
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       0     1176       5  978300760\n",
      "1       0      655       3  978302109\n",
      "2       0      902       3  978301968\n",
      "3       0     3339       4  978300275\n",
      "4       0     2286       5  978824291\n"
     ]
    }
   ],
   "source": [
    "# Codificar los UserID y MovieID a índices consecutivos\n",
    "user2idx = {user_id: idx for idx, user_id in enumerate(users[\"UserID\"].unique())}\n",
    "movie2idx = {movie_id: idx for idx, movie_id in enumerate(movies[\"MovieID\"].unique())}\n",
    "\n",
    "# Aplicar el mapeo a ratings\n",
    "ratings[\"UserID\"] = ratings[\"UserID\"].map(user2idx)\n",
    "ratings[\"MovieID\"] = ratings[\"MovieID\"].map(movie2idx)\n",
    "\n",
    "\n",
    "# Guardar número total de usuarios y películas\n",
    "num_users = len(user2idx)\n",
    "num_movies = len(movie2idx)\n",
    "\n",
    "print(f\"Total usuarios: {num_users}, Total películas: {num_movies}\")\n",
    "print(ratings.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir en Train / Validation / Test\n",
    "\n",
    "    Train (70%) → Para entrenar el modelo.\n",
    "\n",
    "    Validation (15%) → Para ajustar hiperparámetros.\n",
    "\n",
    "    Test (15%) → Para evaluar el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño Train: 700146, Validación: 150031, Test: 150032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento (70%), validación (15%) y prueba (15%)\n",
    "train_data, temp_data = train_test_split(ratings, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Tamaño Train: {len(train_data)}, Validación: {len(val_data)}, Test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear PyTorch Dataset y DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch de usuarios: tensor([4479, 4504, 3490, 2319, 3609])\n",
      "Batch de películas: tensor([2338, 1906, 2204, 1503, 1797])\n",
      "Batch de ratings: tensor([3., 4., 3., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset personalizado para MovieLens 1M\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df[\"UserID\"].values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(df[\"MovieID\"].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df[\"Rating\"].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
    "\n",
    "# Crear datasets para entrenamiento, validación y prueba\n",
    "train_dataset = MovieLensDataset(train_data)\n",
    "val_dataset = MovieLensDataset(val_data)\n",
    "test_dataset = MovieLensDataset(test_data)\n",
    "\n",
    "# Crear DataLoaders para cargar los datos en lotes\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verificar que los DataLoaders funcionan correctamente\n",
    "for users, movies, ratings in train_loader:\n",
    "    print(\"Batch de usuarios:\", users[:5])\n",
    "    print(\"Batch de películas:\", movies[:5])\n",
    "    print(\"Batch de ratings:\", ratings[:5])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedRecommenderNet(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=64):\n",
    "        super(ImprovedRecommenderNet, self).__init__()\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "\n",
    "        # Inicialización de pesos mejorada\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_embedding.weight)\n",
    "\n",
    "        # Capas densas con más capacidad y regularización\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        movie_vec = self.movie_embedding(movie_ids)\n",
    "        x = torch.cat([user_vec, movie_vec], dim=1)\n",
    "\n",
    "        x = self.dropout1(F.leaky_relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout2(F.leaky_relu(self.bn2(self.fc2(x))))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.output(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.9849 - Validation Loss: 0.8676\n",
      "Mejor modelo guardado.\n",
      "Epoch 2/30 - Train Loss: 0.8401 - Validation Loss: 0.8172\n",
      "Mejor modelo guardado.\n",
      "Epoch 3/30 - Train Loss: 0.8084 - Validation Loss: 0.8050\n",
      "Mejor modelo guardado.\n",
      "Epoch 4/30 - Train Loss: 0.7930 - Validation Loss: 0.7947\n",
      "Mejor modelo guardado.\n",
      "Epoch 5/30 - Train Loss: 0.7824 - Validation Loss: 0.7940\n",
      "Mejor modelo guardado.\n",
      "Epoch 6/30 - Train Loss: 0.7744 - Validation Loss: 0.7836\n",
      "Mejor modelo guardado.\n",
      "Epoch 7/30 - Train Loss: 0.7648 - Validation Loss: 0.7734\n",
      "Mejor modelo guardado.\n",
      "Epoch 8/30 - Train Loss: 0.7554 - Validation Loss: 0.7734\n",
      "Epoch 9/30 - Train Loss: 0.7470 - Validation Loss: 0.7762\n",
      "Epoch 10/30 - Train Loss: 0.7405 - Validation Loss: 0.7639\n",
      "Mejor modelo guardado.\n",
      "Epoch 11/30 - Train Loss: 0.7352 - Validation Loss: 0.7689\n",
      "Epoch 12/30 - Train Loss: 0.7308 - Validation Loss: 0.7621\n",
      "Mejor modelo guardado.\n",
      "Epoch 13/30 - Train Loss: 0.7250 - Validation Loss: 0.7635\n",
      "Epoch 14/30 - Train Loss: 0.7217 - Validation Loss: 0.7617\n",
      "Mejor modelo guardado.\n",
      "Epoch 15/30 - Train Loss: 0.7178 - Validation Loss: 0.7594\n",
      "Mejor modelo guardado.\n",
      "Epoch 16/30 - Train Loss: 0.7147 - Validation Loss: 0.7586\n",
      "Mejor modelo guardado.\n",
      "Epoch 17/30 - Train Loss: 0.7109 - Validation Loss: 0.7536\n",
      "Mejor modelo guardado.\n",
      "Epoch 18/30 - Train Loss: 0.7075 - Validation Loss: 0.7538\n",
      "Epoch 19/30 - Train Loss: 0.7044 - Validation Loss: 0.7549\n",
      "Epoch 20/30 - Train Loss: 0.7027 - Validation Loss: 0.7540\n",
      "Epoch 21/30 - Train Loss: 0.6996 - Validation Loss: 0.7602\n",
      "Epoch 22/30 - Train Loss: 0.6974 - Validation Loss: 0.7536\n",
      "No hubo mejora en la pérdida de validación, deteniendo el entrenamiento.\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Inicializamos el modelo\n",
    "model = ImprovedRecommenderNet(num_users, num_movies, embedding_dim=64)\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.MSELoss()  # Error cuadrático medio\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "# Función de entrenamiento con Early Stopping\n",
    "def train_model_early_stopping(model, train_loader, val_loader, criterion, optimizer, epochs=30, patience=5):\n",
    "    best_val_loss = float('inf')  # Mantener la mejor pérdida de validación\n",
    "    epochs_without_improvement = 0  # Contador de épocas sin mejora\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Poner el modelo en modo de entrenamiento\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for users, movies, ratings in train_loader:\n",
    "            optimizer.zero_grad()  # Limpiar los gradientes\n",
    "            \n",
    "            # Hacer las predicciones\n",
    "            predictions = model(users, movies)\n",
    "            \n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(predictions, ratings)\n",
    "            \n",
    "            # Hacer backpropagation y actualizar los pesos\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Promediar la pérdida del entrenamiento\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()  # Modo evaluación\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for users, movies, ratings in val_loader:\n",
    "                predictions = model(users, movies)\n",
    "                loss = criterion(predictions, ratings)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Imprimir resultados\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Guardar el modelo si la pérdida de validación mejora\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Mejor modelo guardado.\")\n",
    "            epochs_without_improvement = 0  # Resetear el contador\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"No hubo mejora en la pérdida de validación, deteniendo el entrenamiento.\")\n",
    "                break\n",
    "    \n",
    "    print(\"Entrenamiento completado.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Entrenar el modelo\n",
    "trained_model = train_model_early_stopping(model, train_loader, val_loader, criterion, optimizer, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImprovedRecommenderNet(\n",
       "  (user_embedding): Embedding(6040, 64)\n",
       "  (movie_embedding): Embedding(3883, 64)\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cargar el modelo guardado best_model.pth\n",
    "\n",
    "best_model = ImprovedRecommenderNet(num_users, num_movies, embedding_dim=64)\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "best_model.eval()  # Modo evaluación\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7511 - MSE: 0.7509\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # Establecer el modelo en modo evaluación\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings in test_loader:\n",
    "            # Hacer las predicciones\n",
    "            predictions = model(users, movies)\n",
    "            \n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(predictions, ratings)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Almacenar las predicciones y las etiquetas reales para calcular métricas\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(ratings.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f} - MSE: {mse:.4f}\")\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_mse = evaluate_model(trained_model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8666\n"
     ]
    }
   ],
   "source": [
    "#calcular RMSE\n",
    "\n",
    "rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
