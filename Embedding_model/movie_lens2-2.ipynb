{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the movieLens dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:\n",
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "\n",
      "Movies:\n",
      "   MovieID                               Title                        Genres\n",
      "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4        5  Father of the Bride Part II (1995)                        Comedy\n",
      "\n",
      "Users:\n",
      "   UserID Gender  Age  Occupation Zip-code\n",
      "0       1      F    1          10    48067\n",
      "1       2      M   56          16    70072\n",
      "2       3      M   25          15    55117\n",
      "3       4      M   45           7    02460\n",
      "4       5      M   25          20    55455\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rutas de los archivos\n",
    "movies_path = \"ml-1m/movies.dat\"\n",
    "ratings_path = \"ml-1m/ratings.dat\"\n",
    "users_path = \"ml-1m/users.dat\"\n",
    "\n",
    "# Carga de los datos\n",
    "# Cargar archivos\n",
    "users = pd.read_csv(\"ml-1m/users.dat\", sep=\"::\", engine=\"python\", \n",
    "                    names=[\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"], encoding=\"latin-1\")\n",
    "\n",
    "movies = pd.read_csv(\"ml-1m/movies.dat\", sep=\"::\", engine=\"python\", \n",
    "                     names=[\"MovieID\", \"Title\", \"Genres\"], encoding=\"latin-1\")\n",
    "\n",
    "ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep=\"::\", engine=\"python\", \n",
    "                      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], encoding=\"latin-1\")\n",
    "\n",
    "# Mostrar primeras filas para verificar\n",
    "print(\"Ratings:\")\n",
    "print(ratings.head())\n",
    "print(\"\\nMovies:\")\n",
    "print(movies.head())\n",
    "print(\"\\nUsers:\")\n",
    "print(users.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usuarios: 6040, Total pel√≠culas: 3883\n"
     ]
    }
   ],
   "source": [
    "# Normalizar ratings de 1-5 a 0-1\n",
    "ratings[\"Rating\"] = (ratings[\"Rating\"] - 1.0) / 4.0\n",
    "\n",
    "# Convertir g√©neros a listas\n",
    "movies[\"Genres\"] = movies[\"Genres\"].apply(lambda x: x.split(\"|\"))\n",
    "\n",
    "# Codificar IDs\n",
    "user2idx = {user_id: idx for idx, user_id in enumerate(users[\"UserID\"].unique())}\n",
    "movie2idx = {movie_id: idx for idx, movie_id in enumerate(movies[\"MovieID\"].unique())}\n",
    "\n",
    "ratings[\"UserID\"] = ratings[\"UserID\"].map(user2idx)\n",
    "ratings[\"MovieID\"] = ratings[\"MovieID\"].map(movie2idx)\n",
    "\n",
    "num_users = len(user2idx)\n",
    "num_movies = len(movie2idx)\n",
    "\n",
    "print(f\"Total usuarios: {num_users}, Total pel√≠culas: {num_movies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir en Train / Validation / Test\n",
    "\n",
    "    Train (70%) ‚Üí Para entrenar el modelo.\n",
    "\n",
    "    Validation (15%) ‚Üí Para ajustar hiperpar√°metros.\n",
    "\n",
    "    Test (15%) ‚Üí Para evaluar el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o Train: 700146, Validaci√≥n: 150031, Test: 150032\n"
     ]
    }
   ],
   "source": [
    "# Divisi√≥n: 70% Train, 15% Val, 15% Test\n",
    "train_data, temp_data = train_test_split(ratings, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Tama√±o Train: {len(train_data)}, Validaci√≥n: {len(val_data)}, Test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear PyTorch Dataset y DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df[\"UserID\"].values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(df[\"MovieID\"].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df[\"Rating\"].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
    "\n",
    "# Instanciar datasets\n",
    "train_dataset = MovieLensDataset(train_data)\n",
    "val_dataset = MovieLensDataset(val_data)\n",
    "test_dataset = MovieLensDataset(test_data)\n",
    "\n",
    "# Loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedRecommenderNet(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=64):\n",
    "        super(ImprovedRecommenderNet, self).__init__()\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "\n",
    "        # Inicializaci√≥n de pesos mejorada\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_embedding.weight)\n",
    "\n",
    "        # Capas densas con m√°s capacidad y regularizaci√≥n\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        movie_vec = self.movie_embedding(movie_ids)\n",
    "        x = torch.cat([user_vec, movie_vec], dim=1)\n",
    "\n",
    "        x = self.dropout1(F.leaky_relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout2(F.leaky_relu(self.bn2(self.fc2(x))))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.output(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperRecommenderNet(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=128):\n",
    "        super(DeeperRecommenderNet, self).__init__()\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "\n",
    "        # Inicializaci√≥n Xavier\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_embedding.weight)\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 512)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.output = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        movie_vec = self.movie_embedding(movie_ids)\n",
    "\n",
    "        x = torch.cat([user_vec, movie_vec], dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia del modelo\n",
    "model = DeeperRecommenderNet(num_users, num_movies, embedding_dim=128)\n",
    "\n",
    "# Funci√≥n de p√©rdida\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizador con weight decay para regularizaci√≥n L2\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50, patience=5, clip_value=1.0):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for users, movies, ratings in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip de gradientes\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validaci√≥n\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for users, movies, ratings in val_loader:\n",
    "                predictions = model(users, movies)\n",
    "                loss = criterion(predictions, ratings)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"üü¢ Mejor modelo guardado.\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"üõë Early stopping: no mejora en validaci√≥n.\")\n",
    "                break\n",
    "\n",
    "    print(\"‚úÖ Entrenamiento finalizado.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.0592 | Val Loss: 0.0536\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 2/50 | Train Loss: 0.0536 | Val Loss: 0.0521\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 3/50 | Train Loss: 0.0523 | Val Loss: 0.0518\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 4/50 | Train Loss: 0.0518 | Val Loss: 0.0514\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 5/50 | Train Loss: 0.0513 | Val Loss: 0.0509\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 6/50 | Train Loss: 0.0506 | Val Loss: 0.0504\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 7/50 | Train Loss: 0.0502 | Val Loss: 0.0503\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 8/50 | Train Loss: 0.0501 | Val Loss: 0.0501\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 9/50 | Train Loss: 0.0500 | Val Loss: 0.0502\n",
      "Epoch 10/50 | Train Loss: 0.0500 | Val Loss: 0.0502\n",
      "Epoch 11/50 | Train Loss: 0.0500 | Val Loss: 0.0500\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 12/50 | Train Loss: 0.0499 | Val Loss: 0.0498\n",
      "üü¢ Mejor modelo guardado.\n",
      "Epoch 13/50 | Train Loss: 0.0499 | Val Loss: 0.0499\n",
      "Epoch 14/50 | Train Loss: 0.0500 | Val Loss: 0.0506\n",
      "Epoch 15/50 | Train Loss: 0.0499 | Val Loss: 0.0501\n",
      "Epoch 16/50 | Train Loss: 0.0499 | Val Loss: 0.0498\n",
      "Epoch 17/50 | Train Loss: 0.0499 | Val Loss: 0.0503\n",
      "üõë Early stopping: no mejora en validaci√≥n.\n",
      "‚úÖ Entrenamiento finalizado.\n"
     ]
    }
   ],
   "source": [
    "# Entrenar\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeeperRecommenderNet(\n",
       "  (user_embedding): Embedding(6040, 128)\n",
       "  (movie_embedding): Embedding(3883, 128)\n",
       "  (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (dropout1): Dropout(p=0.4, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (dropout3): Dropout(p=0.2, inplace=False)\n",
       "  (output): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear una nueva instancia y cargar pesos\n",
    "best_model = DeeperRecommenderNet(num_users, num_movies, embedding_dim=64)\n",
    "best_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "best_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings in test_loader:\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(ratings.cpu().numpy())\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "\n",
    "    # Convertir a numpy arrays\n",
    "    all_preds = np.clip(np.array(all_preds), 0.0, 1.0)  # clipping\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Escala original [1‚Äì5]\n",
    "    preds_original = all_preds * 4 + 1\n",
    "    labels_original = all_labels * 4 + 1\n",
    "\n",
    "    mse = mean_squared_error(labels_original, preds_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(labels_original, preds_original)\n",
    "\n",
    "    print(f\"üîç Test MSE (1-5): {mse:.4f}\")\n",
    "    print(f\"üìâ Test RMSE (1-5): {rmse:.4f}\")\n",
    "    print(f\"üìä Test MAE  (1-5): {mae:.4f}\")\n",
    "    \n",
    "    return mse, rmse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Test MSE (1-5): 0.7963\n",
      "üìâ Test RMSE (1-5): 0.8923\n",
      "üìä Test MAE  (1-5): 0.7071\n"
     ]
    }
   ],
   "source": [
    "mse, rmse, mae = evaluate_model(best_model, test_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
