{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the movieLens dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:\n",
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "\n",
      "Movies:\n",
      "   MovieID                               Title                        Genres\n",
      "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4        5  Father of the Bride Part II (1995)                        Comedy\n",
      "\n",
      "Users:\n",
      "   UserID Gender  Age  Occupation Zip-code\n",
      "0       1      F    1          10    48067\n",
      "1       2      M   56          16    70072\n",
      "2       3      M   25          15    55117\n",
      "3       4      M   45           7    02460\n",
      "4       5      M   25          20    55455\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rutas de los archivos\n",
    "movies_path = \"../ml-1m/movies.dat\"\n",
    "ratings_path = \"../ml-1m/ratings.dat\"\n",
    "users_path = \"../ml-1m/users.dat\"\n",
    "\n",
    "# Carga de los datos\n",
    "# Cargar archivos\n",
    "users = pd.read_csv(\"../ml-1m/users.dat\", sep=\"::\", engine=\"python\", \n",
    "                    names=[\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"], encoding=\"latin-1\")\n",
    "\n",
    "movies = pd.read_csv(\"../ml-1m/movies.dat\", sep=\"::\", engine=\"python\", \n",
    "                     names=[\"MovieID\", \"Title\", \"Genres\"], encoding=\"latin-1\")\n",
    "\n",
    "ratings = pd.read_csv(\"../ml-1m/ratings.dat\", sep=\"::\", engine=\"python\", \n",
    "                      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], encoding=\"latin-1\")\n",
    "\n",
    "# Mostrar primeras filas para verificar\n",
    "print(\"Ratings:\")\n",
    "print(ratings.head())\n",
    "print(\"\\nMovies:\")\n",
    "print(movies.head())\n",
    "print(\"\\nUsers:\")\n",
    "print(users.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usuarios: 6040, Total pel√≠culas: 3883\n"
     ]
    }
   ],
   "source": [
    "# Normalizar ratings de 1-5 a 0-1\n",
    "ratings[\"Rating\"] = (ratings[\"Rating\"] - 1.0) / 4.0\n",
    "\n",
    "# Convertir g√©neros a listas\n",
    "movies[\"Genres\"] = movies[\"Genres\"].apply(lambda x: x.split(\"|\"))\n",
    "\n",
    "# Codificar IDs\n",
    "user2idx = {user_id: idx for idx, user_id in enumerate(users[\"UserID\"].unique())}\n",
    "movie2idx = {movie_id: idx for idx, movie_id in enumerate(movies[\"MovieID\"].unique())}\n",
    "\n",
    "ratings[\"UserID\"] = ratings[\"UserID\"].map(user2idx)\n",
    "ratings[\"MovieID\"] = ratings[\"MovieID\"].map(movie2idx)\n",
    "\n",
    "num_users = len(user2idx)\n",
    "num_movies = len(movie2idx)\n",
    "\n",
    "print(f\"Total usuarios: {num_users}, Total pel√≠culas: {num_movies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir en Train / Validation / Test\n",
    "\n",
    "    Train (70%) ‚Üí Para entrenar el modelo.\n",
    "\n",
    "    Validation (15%) ‚Üí Para ajustar hiperpar√°metros.\n",
    "\n",
    "    Test (15%) ‚Üí Para evaluar el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o Train: 700146, Validaci√≥n: 150031, Test: 150032\n"
     ]
    }
   ],
   "source": [
    "# Divisi√≥n: 70% Train, 15% Val, 15% Test\n",
    "train_data, temp_data = train_test_split(ratings, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Tama√±o Train: {len(train_data)}, Validaci√≥n: {len(val_data)}, Test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear PyTorch Dataset y DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df[\"UserID\"].values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(df[\"MovieID\"].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df[\"Rating\"].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
    "\n",
    "# Instanciar datasets\n",
    "train_dataset = MovieLensDataset(train_data)\n",
    "val_dataset = MovieLensDataset(val_data)\n",
    "test_dataset = MovieLensDataset(test_data)\n",
    "\n",
    "# Loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedRecommenderNet(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=64):\n",
    "        super(ImprovedRecommenderNet, self).__init__()\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "\n",
    "        # Inicializaci√≥n de pesos mejorada\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_embedding.weight)\n",
    "\n",
    "        # Capas densas con m√°s capacidad y regularizaci√≥n\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        movie_vec = self.movie_embedding(movie_ids)\n",
    "        x = torch.cat([user_vec, movie_vec], dim=1)\n",
    "\n",
    "        x = self.dropout1(F.leaky_relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout2(F.leaky_relu(self.bn2(self.fc2(x))))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.output(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperRecommenderNet(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=128):\n",
    "        super(DeeperRecommenderNet, self).__init__()\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "\n",
    "        # Inicializaci√≥n Xavier\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_embedding.weight)\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 512)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.output = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        movie_vec = self.movie_embedding(movie_ids)\n",
    "\n",
    "        x = torch.cat([user_vec, movie_vec], dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia del modelo\n",
    "model = DeeperRecommenderNet(num_users, num_movies, embedding_dim=128)\n",
    "\n",
    "# Funci√≥n de p√©rdida\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizador con weight decay para regularizaci√≥n L2\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50, patience=5, clip_value=1.0):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for users, movies, ratings in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip de gradientes\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validaci√≥n\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for users, movies, ratings in val_loader:\n",
    "                predictions = model(users, movies)\n",
    "                loss = criterion(predictions, ratings)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"üü¢ Mejor modelo guardado.\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"üõë Early stopping: no mejora en validaci√≥n.\")\n",
    "                break\n",
    "\n",
    "    print(\"‚úÖ Entrenamiento finalizado.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.0590 | Val Loss: 0.0536\n",
      "üü¢ Mejor modelo guardado.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trained_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, epochs, patience, clip_value)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m users, movies, ratings \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43musers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmovies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m         loss = criterion(predictions, ratings)\n\u001b[32m     30\u001b[39m         val_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/DeepLearning/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/DeepLearning/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mDeeperRecommenderNet.forward\u001b[39m\u001b[34m(self, user_ids, movie_ids)\u001b[39m\n\u001b[32m     25\u001b[39m movie_vec = \u001b[38;5;28mself\u001b[39m.movie_embedding(movie_ids)\n\u001b[32m     27\u001b[39m x = torch.cat([user_vec, movie_vec], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout1(x)\n\u001b[32m     32\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.fc2(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/DeepLearning/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1704\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1702\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Entrenar\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeeperRecommenderNet(\n",
       "  (user_embedding): Embedding(6040, 128)\n",
       "  (movie_embedding): Embedding(3883, 128)\n",
       "  (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (dropout1): Dropout(p=0.4, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (dropout3): Dropout(p=0.2, inplace=False)\n",
       "  (output): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear una nueva instancia y cargar pesos\n",
    "best_model = DeeperRecommenderNet(num_users, num_movies, embedding_dim=128)\n",
    "best_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "best_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings in test_loader:\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(ratings.cpu().numpy())\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "\n",
    "    # Convertir a numpy arrays\n",
    "    all_preds = np.clip(np.array(all_preds), 0.0, 1.0)  # clipping\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Escala original [1‚Äì5]\n",
    "    preds_original = all_preds * 4 + 1\n",
    "    labels_original = all_labels * 4 + 1\n",
    "\n",
    "    mse = mean_squared_error(labels_original, preds_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(labels_original, preds_original)\n",
    "\n",
    "    print(f\"üîç Test MSE (1-5): {mse:.4f}\")\n",
    "    print(f\"üìâ Test RMSE (1-5): {rmse:.4f}\")\n",
    "    print(f\"üìä Test MAE  (1-5): {mae:.4f}\")\n",
    "    \n",
    "    return mse, rmse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Test MSE (1-5): 0.7963\n",
      "üìâ Test RMSE (1-5): 0.8923\n",
      "üìä Test MAE  (1-5): 0.7071\n"
     ]
    }
   ],
   "source": [
    "mse, rmse, mae = evaluate_model(best_model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_original_id, model, movies_df, ratings_df, user2idx, movie2idx, top_k=10):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert original user ID to internal index\n",
    "    user_idx = user2idx.get(user_original_id)\n",
    "    if user_idx is None:\n",
    "        print(\"‚ö†Ô∏è User not found.\")\n",
    "        return\n",
    "\n",
    "    # Get movies the user has already rated\n",
    "    seen_movies = ratings_df[ratings_df[\"UserID\"] == user_idx][\"MovieID\"].values\n",
    "\n",
    "    # List of movies the user hasn't seen yet\n",
    "    unseen_movies = [mid for mid in movie2idx.values() if mid not in seen_movies]\n",
    "\n",
    "    # Create tensors\n",
    "    user_tensor = torch.tensor([user_idx] * len(unseen_movies), dtype=torch.long)\n",
    "    movie_tensor = torch.tensor(unseen_movies, dtype=torch.long)\n",
    "\n",
    "    # Predict ratings\n",
    "    with torch.no_grad():\n",
    "        predictions = model(user_tensor, movie_tensor)\n",
    "        predictions = torch.clamp(predictions, 0.0, 1.0)  # ensure in [0, 1]\n",
    "        predicted_scores = predictions.numpy() * 4 + 1  # rescale to [1, 5]\n",
    "\n",
    "    # Get top K movie indices\n",
    "    top_indices = np.argsort(predicted_scores)[-top_k:][::-1]\n",
    "    top_movie_ids = [\n",
    "        list(movie2idx.keys())[list(movie2idx.values()).index(unseen_movies[i])]\n",
    "        for i in top_indices\n",
    "    ]\n",
    "    top_scores = [predicted_scores[i] for i in top_indices]\n",
    "\n",
    "    # Print recommendations\n",
    "    print(f\"\\nüé¨ Top {top_k} movie recommendations for user {user_original_id}:\\n\")\n",
    "    for title, score in zip(movies_df[movies_df[\"MovieID\"].isin(top_movie_ids)][\"Title\"], top_scores):\n",
    "        print(f\"‚≠ê {title} - Predicted Rating: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé¨ Top 10 movie recommendations for user 75:\n",
      "\n",
      "‚≠ê Usual Suspects, The (1995) - Predicted Rating: 4.89\n",
      "‚≠ê Shawshank Redemption, The (1994) - Predicted Rating: 4.83\n",
      "‚≠ê Schindler's List (1993) - Predicted Rating: 4.78\n",
      "‚≠ê Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963) - Predicted Rating: 4.78\n",
      "‚≠ê Godfather, The (1972) - Predicted Rating: 4.78\n",
      "‚≠ê Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) - Predicted Rating: 4.73\n",
      "‚≠ê Raiders of the Lost Ark (1981) - Predicted Rating: 4.73\n",
      "‚≠ê To Kill a Mockingbird (1962) - Predicted Rating: 4.73\n",
      "‚≠ê Third Man, The (1949) - Predicted Rating: 4.73\n",
      "‚≠ê Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954) - Predicted Rating: 4.71\n"
     ]
    }
   ],
   "source": [
    "recommend_movies(75, best_model, movies, ratings, user2idx, movie2idx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
